# Minecraft agent evaluation

This module contains the scripts used to create and manage Amazon Mturk HITs used
to pair human Gamers with Agents in Minecraft. The objective is to collect
data of the agents behavior, and human appreciation over its general performance.

## How does data collection works

The Minecraft Data Collection Platform provides *join codes* to pair each task with each deployed
agent. These *join codes* are included into the HTML layout of the HIT, where the turker can copy them and
join the Minecraft game.

### Step 1: Encode and upload the tasks into the Minecraft Data Collection Platform.


The minecraft data collection platform receives tasks defined in a structured json format.
To facilitate the conversion of tasks to this format, we use the `gridworld` libraries Task abstractions.

You may import `convert_iglu_tasks_to_minecraft_format.py` and call `convert` as shown below.
The convert function will return a json string with the task data. This json can be uploaded
directly to the Minecraft Data Collection Platform.

```python
from gridworld_task_to_minecraft_task_encoder import GridworldTaskToMinecraftTaskEncoder
from gridworld.data import SingleTurnIGLUDataset

iglu_dataset = SingleTurnIGLUDataset()

encoded_tasks = []
task_encoder = GridworldTaskToJsonCompleteTaskEncoder(
    "<ARCHITECT ROLE ID FROM MINECRAFT DATA COLLECTION PLATFORM>",
    "<BUILDER ROLE ID FROM MINECRAFT DATA COLLECTION PLATFORM>",
    "<path-to>/data_formats/task_definition_json_format.json.j2")

for (task_name, task_group) in iglu_dataset.tasks.items():
  for task in task_group:
    encoded_tasks.append(
      task_encoder.convert(
        task_name=task_name,
        task=task,
        task_state="Published",
        world_size_x=160,
        world_size_z=160,
        game_limit_max_duration_seconds=(5 * 60),
        game_limit_max_turns=10,
        turn_limits={
            "<ROLE ID>": (3 * 60)
        }
      )
    )
```

Additionally, the `convert_iglu_tasks_to_minecraft_format.py` uses a mechanism similar to the
one above to convert a hard-coded set of tasks that need to be defined inside the file. The resulting
task json strings are directly stored in files inside `output_dirpath`. The name of the file is either the task id or a custom name for each task provided inside the code.

```bash
$ python ./minecraft_evaluation/task_encoding/convert_iglu_tasks_to_minecraft_format.py \
  --builder_role_id <BUILDER ROLE ID FOR TOURNAMENT> \
  --architect_role_id <ARCHITECT ROLE ID FOR TOURNAMENT> \
  --output_dirpath test_new_tasks \
  --task_template_filepath ./minecraft_evaluation/task_encoding/minecraft_task_template.json.j2
```

Some modifications are introduced by the rendered to the block structure of the task are:

1. The blocks to build the grid on the floor and gray letters signaling the cardinal points are added as blockChanges, along with the blocks in the starting grid structure.
2. The coordinates of the blocks are shifted differently:
  1. Blocks in starting grid are already represented in a sparse format. Only the y coordinate is increased by two so the lowest value is 1. Using the default grass generator, the floor is positioned at y=0. X and z axis values are centered in the 0,0 position, and range between -5 to 5.
  2. Blocks in the target grid are represented as a dense matrix of 11x11x9 blocks. They are converted to sparse format and re-centered to the 0,0,1 coordinate as the starting grid.

Material ids are also transformed from gridworld indexes (1 = blue, 2 = green, etc.) to real Minecraft materials. Colored wool is used to mimic gridworld blocks.

### Step 2: Obtaining the join codes

Through Minecraft Data Collection Platform two json files with join codes will can downloaded. The first one maps the task names
to the task ids generated by the platform

```json
{
    "taskIdNamesList": [
        {
            "id": "2d62a767-ad64-4d49-8dfa-1d68de8f9b6f",
            "name": "c139 - task 1"
        },
        â€¦
    ]
}
```

The second one maps the previous task ids to the specific join commands for each agent

```json
{
    "info": {
        "tournamentId": <id>,
        "challengeId": <id of agent challenge?>,
        "roleId": <id - builder>,
        "taskIds": [
            # Ids generated by the dashboard for each uploaded task
            "0047d731-52a1-45b7-bb1b-9a9712d64a43",
            ...
        ],
        "agents": [
            {
                "id": "26a5ebc3-b6f3-4b43-b77c-87327e8c702f",
                "name": "baseline-agent-for-deployed-agent-test"
            }
        ]
    },
    "commands": [
        # Commands to join each task with this agent service
        "/plaiground:join-task-with-agent 67df9eb9-0d67-4cce-b1db-c8e565ec4e8e:491bf7ed-2982-4730-bbe9-4062af69f4a5:0047d731-52a1-45b7-bb1b-9a9712d64a43:26a5ebc3-b6f3-4b43-b77c-87327e8c702f",
        ...
    ]
}
```

The following script will clean the previous information into a file with the codes to join
each task. Then the agent id can be added on demand.

```bash
python ./minecraft_evaluation/task_management/join_codes_generator.py \
  --task_id_mapping <name_to_minecraft_id_mapping_filepath.json> \
  --join_codes_filepath <agent_challenge_code_info_filepath.json> \
  --output_codes_filepath <output_filepath>
```

This will generate a single json file with structure, that can be used as input
to the TaskManager class to obtain balanced pairs of tasks and agents.

```json
{
    "<task_name>": {
        "join_code": "<task_join_code>",
        "task_id": "<minecrat_task_id>",
    }
}
```

The second file you will need is a file grouping multiple instances that run the
same agent into a common name, and must have the following structure:

```json
{
    "<agent_name>": [{
        "agent_instance_id": "<minecraft_agent_instance_id>",
        "agent_instance_name": "agent1_instance1"
    }, {
        "agent_instance_id": "<minecraft_agent_instance_id>",
        "agent_instance_name": "agent2_instance1"
    }]
}
```

### Step 2: Create the HITs

An HTML/XML template will be used as layout when creating the HITs and should be compatible with the scripts to run in the following step. For example, the singleagent script requires the following fields:

The HTML layout must contain at least:
1. The join code(s)
2. A field to paste the completion code(s).
3. A hidden JoinCode field to send the original join code into the assignment answer.

With the layout and the values on the `env_configs.json` file under key `hit_config`, a new HIT can be created.
The current implementation only supports a single assignment per HIT.
The newly created HIT id is added to a `Game` instances that keeps track of the task_id, agent, join code and expiration date.

In this step, a new entry on the game storage using class `MinecraftEvalStorage` to keep track of open and validated assignments.
Entries are encoded into the format expected by the game storage from the
`Game` instance associated to the HIT, through methods `to_database_entry` and `from_database_entry`.

### Step 3: Monitor and approve/reject the completed assignments

When the assignment is complete, the Mturker will introduce a completion code that will be used
to download the game data associated with the provided join code.

Scripts such as `run_single_agent_data_collection.py` and `run_pairwise_data_collection.py` execute an infinite loop that waits for new completed assignments, until all the necessary data has been collected. The scripts receive the json files created in the previous step with the join codes.

Once the assignment is read, its entry on the database is updated to include the Mturker
responses for the questions included in the HIT, the generated *completion code*,
the validation value of the HIT, and the worker ID. In a posterior process, the
worker ID will be hashed to avoid leaking the raw data.

#### Step 3.2: Download the game data and check for quality

Using the completion code, the sorted list of Events produced during the game and other game metadata can be downloaded from an Azure Storage Container provided by the data collection platform. The class `GameEventHandler` contains helper methods to parse the data and iterate over it. The list of events can be used for HIT validation, for example, by counting the number of turns that change between players.

Downloading the game data is an expensive procedure. In this same step, the data will be cleaned of PII
information and moved into to a private container.

### Step 4: Post process data


## Multi-agent HITs

Comparative HITs instruct the annotator to play against several agents, and then choose which one they prefer. For the same HIT, two or more join codes are provided and, for each of them, one game with a corresponding confirmation codes will be created. Each game will be stored into a different row in the database, that will share the same HITId.

## Continuos data collection

This repository includes scripts that monitor collected data automatically. Using a while loop, HITs are created whenever there is a pair of agent instances free, until the desired number of assignments has been completed. Tasks and agent pairs are selected in such a way to ensures a balanced number of results for each combination.
This allows for optimal data coverage and usage of agent instances without the need of human supervision.

The scripts that use this strategy satisfy the following conditions:
1. No two HITs are created using the same instance.
2. When a HIT is completed, the assigned agent instances are enqueued for reuse.
3. Agent combinations for the same HIT are selected giving priority to the ones with the least number of previous hits, over all tasks.
4. A task is not returned again for the same agent combination unless all previous task were previously used.

The script `run_single_agent_data_collection.py` creates hits with only one join code. It receives the json files with task join codes and agent instances described above. It generates one hit for each task/agent combination.

The script `run_pairwise_data_collection.py` does something similar, but each hit combines two
agents.


## Set up

The current version of the repository supports Python3.9 or above.

Create an environment with the tool of your choice and install the dependencies in `requirements.txt`. Additional packages to run the notebooks
are provided in a separate `notebooks/requirements.txt` file. Example:

bash
```
$ conda create --name iglu_dataset python=3.9
$ conda activate iglu_dataset
$ pip install -r ../requirements.txt
$ pip install -r notebooks/requirements.txt
```

Finally, install gridworld package from sources:

bash
```
$ pip install git+https://github.com/iglu-contest/gridworld.git@master
```

